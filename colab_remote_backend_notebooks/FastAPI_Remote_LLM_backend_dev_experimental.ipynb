{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYecE9iPpGRy"
      },
      "source": [
        "# Required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtdwVCeq1wcT"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oagu4XXW3rx-"
      },
      "outputs": [],
      "source": [
        "# Takes about 2 minutes\n",
        "!pip install -q fastapi uvicorn pyngrok pydantic bitsandbytes>=0.39.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1iYVcPWBxqz"
      },
      "outputs": [],
      "source": [
        "# Compile and install with CPU support (Takes about 3 minutes)\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RRqnDyj4xGoN"
      },
      "outputs": [],
      "source": [
        "# Compile and install with GPU support (Takes about 30 minutes)\n",
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install -v llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykF4OyYACX7T"
      },
      "outputs": [],
      "source": [
        "# Prebuilt for CPU (Takes less than a minute) (Doesn't work on colab)\n",
        "!pip install llama-cpp-python==0.3.2 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
        "# Don't change version !!! (Not all versions have prebuilt wheels, check link above for latest version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ItWMwSPq1uz3",
        "outputId": "c76017b3-58a0-4085-84ce-59351b0520ec"
      },
      "outputs": [],
      "source": [
        "# Prebuilt for GPU (Takes less than a minute)\n",
        "!pip install -q llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
        "# Don't change version !!! (Not all versions have prebuilt wheels, check link above for latest version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNUz1vu2pCEt"
      },
      "source": [
        "# Download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT6QBxc6wjyu",
        "outputId": "47ab77dc-0a8b-44d6-9144-c6ac3510a18b"
      },
      "outputs": [],
      "source": [
        "# from llama_cpp import Llama\n",
        "# llm = Llama.from_pretrained(\n",
        "#     repo_id=\"lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
        "#     filename=\"*Q4_K_M.gguf\",\n",
        "#     verbose=False\n",
        "# )\n",
        "\n",
        "# with aria2\n",
        "# !apt-get -y install -qq aria2\n",
        "# !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/mradermacher/Xwen-7B-Chat-GGUF/resolve/main/Xwen-7B-Chat.Q5_K_M.gguf?download=true -d /content/text-generation-webui/models/ -o Xwen-7B-Chat.Q5_K_M.gguf\n",
        "\n",
        "!mkdir -p models\n",
        "!wget -O models/qwen2.5-7b-q4_k_m.gguf https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/resolve/main/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjUT75cKAEfN"
      },
      "source": [
        "# Prepare the app.py file that has the LLM initialization and API methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGiykGER871T"
      },
      "source": [
        "## Imports and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t0u_7jC84JV",
        "outputId": "59b3aeb1-1d37-4b37-ac40-fdfd85e42cba"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import random\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from llama_cpp import Llama\n",
        "import uvicorn\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# Load the model\n",
        "model_path = \"models/qwen2.5-7b-q4_k_m.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOePB29wxWRQ"
      },
      "source": [
        "## Conversation class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgZePSRXxZ1o",
        "outputId": "bfdba128-51ef-4199-9696-f9a7e6743fa2"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "class Conversation:\n",
        "    def __init__(self, llm: Llama, system_prompt=\"\", history=[]):\n",
        "        self.llm = llm\n",
        "        self.system_prompt = system_prompt\n",
        "        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n",
        "    def create_completion(self, user_prompt=''):\n",
        "        # Add the user prompt to the history\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "        # Send the history messages to the LLM\n",
        "        output = self.llm.create_chat_completion(messages=self.history, temperature=0.4, max_tokens=500)\n",
        "        conversation_result = output['choices'][0]['message']\n",
        "        # Append the conversation_result to the history\n",
        "        self.history.append(conversation_result)\n",
        "        return conversation_result['content']\n",
        "\n",
        "    def final_response(self, response_format):\n",
        "        output = self.llm.create_chat_completion(messages=self.history, response_format=response_format)\n",
        "        try:\n",
        "            return json.loads(output[\"choices\"][0][\"message\"][\"content\"].strip())  # Parse JSON response\n",
        "        except json.JSONDecodeError:\n",
        "            return [{\"condition\": \"Error parsing response\", \"confidence\": \"N/A\",\"output\":output}]\n",
        "\n",
        "    def delete_history(self):\n",
        "        self.history=[{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zree4rxF5NbE"
      },
      "source": [
        "## Initializing Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkT24M0-5NJ7",
        "outputId": "b30311e4-ac00-4680-9ea3-1837474dfd1a"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "system_prompt=\"You are a helpful dentist assistant that asks precise questions to diagnose dental conditions based on conversations.\"\n",
        "chatbot = Conversation(llm, system_prompt=system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp4cZme39DK4"
      },
      "source": [
        "## FastAPI initialization and API methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQDGrszg9G9o",
        "outputId": "e5a7e69c-77c0-458e-cd6f-fa205570103d"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:5173\", \"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class SymptomRequest(BaseModel):\n",
        "    symptoms: list[str]\n",
        "    additional_details: str = \"None\"\n",
        "\n",
        "class DiagnosisResponse(BaseModel):\n",
        "    diagnosis: list[dict]\n",
        "\n",
        "class ChatHistoryRequest(BaseModel):\n",
        "    chat_history: list[dict]\n",
        "\n",
        "class SymptomsResponse(BaseModel):\n",
        "    symptoms: list[dict]\n",
        "\n",
        "class ConditionsResponse(BaseModel):\n",
        "    diagnosis: list[dict]\n",
        "\n",
        "class SummaryResponse(BaseModel):\n",
        "    summary: str\n",
        "\n",
        "class CombinedResponse(BaseModel):\n",
        "    symptoms: list[dict]\n",
        "    conditions: list[dict]\n",
        "    summary: str\n",
        "\n",
        "@app.get(\"/hello\")\n",
        "async def root():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/diagnose-en\", response_model=DiagnosisResponse)\n",
        "async def diagnose_patient_lm(request: SymptomRequest):\n",
        "    try:\n",
        "        messages = prepare_prompt_en(request.symptoms, request.additional_details)\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response = query_local_model(messages, response_format)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/diagnose-fr\", response_model=DiagnosisResponse)\n",
        "async def diagnose_patient_fr(request: SymptomRequest):\n",
        "    try:\n",
        "        messages = prepare_prompt_fr(request.symptoms, request.additional_details)\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response = query_local_model(messages, response_format)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/converse\")\n",
        "async def converse(request):\n",
        "    try:\n",
        "        response=chatbot.create_completion(request)\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/converse_diagnose\")\n",
        "async def converse_diagnose():\n",
        "    try:\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response=chatbot.final_response(response_format)\n",
        "        chatbot.delete_history()\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/converse_history\")\n",
        "async def converse_history():\n",
        "    try:\n",
        "        response = chatbot.get_history()\n",
        "        return {\"history\": response[1:]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/random_greeting\")\n",
        "async def get_random_greeting():\n",
        "    greetings=[\"Hello! Welcome to our practice. How can I assist you today?\",\n",
        "           \"Good morning/afternoon! Thank you for visiting us. What brings you in today?\",\n",
        "           \"Hi there! I hope you're feeling well. How can I help you?\",\n",
        "           \"Hello! We're glad you're here. What can I do for you today?\",\n",
        "           \"Good day! I see you're here. What would you like to discuss?\",\n",
        "           \"Hi! Thank you for coming in. How can I assist you?\",\n",
        "           \"Hello! We're here to help. What can I do for you today?\",\n",
        "           \"Hi! I hope you're feeling well. What can I do for you today?\",\n",
        "           \"Good day! We're glad you're here. What brings you in today?\"]\n",
        "    try:\n",
        "        return {\"greeting\": random.choice(greetings)}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51LqNgPM9h2z"
      },
      "source": [
        "### Chat summary, symptoms and conditions FastAPI endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mELeGbh29tzc",
        "outputId": "bfa8cfbb-57e2-4edd-dd32-06c573ba2141"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "@app.post(\"/extract_symptoms\", response_model=SymptomsResponse)\n",
        "async def extract_symptoms(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        response = extract_symptoms_from_history(request.chat_history)\n",
        "        return {\"symptoms\": response[\"symptoms\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/extract_conditions\", response_model=ConditionsResponse)\n",
        "async def extract_conditions(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        response = extract_conditions_from_history(request.chat_history)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/summarize\", response_model=SummaryResponse)\n",
        "async def summarize_chat(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        SummaryResponse.summary = summarize_chat_history(request.chat_history)\n",
        "        return SummaryResponse\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Updated /process_chat endpoint using helper functions\n",
        "@app.post(\"/process_chat\", response_model=CombinedResponse)\n",
        "async def process_chat(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        # Extract all required data using helper functions\n",
        "        symptoms_response = extract_symptoms_from_history(request.chat_history)\n",
        "        conditions_response = extract_conditions_from_history(request.chat_history)\n",
        "        summary = summarize_chat_history(request.chat_history)\n",
        "\n",
        "        # Combine results\n",
        "        return CombinedResponse(\n",
        "            symptoms=symptoms_response.get(\"symptoms\", []),\n",
        "            conditions=conditions_response.get(\"diagnosis\", []),\n",
        "            summary=summary\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_with_model(request: ChatHistoryRequest):\n",
        "    chat_system_prompt = \"You are a helpful dentist assistant that asks precise questions to diagnose dental conditions based on conversations.\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": chat_system_prompt}]\n",
        "        + request.chat_history\n",
        "    )\n",
        "    response = query_local_model(messages)\n",
        "    return {\"response\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVo9OvihRzct"
      },
      "source": [
        "### Improve doctor note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCiqnQgiR1Pn",
        "outputId": "0ba79437-fc2f-420f-e77c-4d35c7f8c65d"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "# Request model for the note improvement endpoint\n",
        "class ImproveNoteRequest(BaseModel):\n",
        "    etat: str\n",
        "    doctor_note: str\n",
        "    chat_history: list[dict]\n",
        "\n",
        "# Response model for the improved note\n",
        "class ImprovedNoteResponse(BaseModel):\n",
        "    improved_note: str\n",
        "\n",
        "# Helper function to improve the doctor's note\n",
        "def improve_doctor_note(etat: str, doctor_note: str, chat_history: list[dict]) -> str:\n",
        "    system_prompt = (\n",
        "        \"You are a professional medical assistant specializing in refining doctor’s notes. \"\n",
        "        \"Your task is to improve the clarity, grammar, and readability of the provided doctor’s note \"\n",
        "        \"while preserving its original meaning and intent. Do not add new information, remove content, \"\n",
        "        \"or alter the medical conclusions. Fix typos, adjust awkward phrasing, and ensure a polished, \"\n",
        "        \"professional tone. Use the consultation status and conversation for context, but do not incorporate \"\n",
        "        \"details from them into the note unless they are already present in the doctor’s note.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"The consultation has the status: {etat}.\\n\"\n",
        "        f\"Here is the doctor’s note to improve:\\n{doctor_note}\\n\"\n",
        "        \"Please provide the improved version of the note, keeping the meaning unchanged.\"\n",
        "    )\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    )\n",
        "    return query_local_model(messages)  # Returns plain text\n",
        "\n",
        "# New endpoint to improve the doctor's note\n",
        "@app.post(\"/improve_note\", response_model=ImprovedNoteResponse)\n",
        "async def improve_note(request: ImproveNoteRequest):\n",
        "    try:\n",
        "        improved_note = improve_doctor_note(\n",
        "            etat=request.etat,\n",
        "            doctor_note=request.doctor_note,\n",
        "            chat_history=request.chat_history\n",
        "        )\n",
        "        return {\"improved_note\": improved_note}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRgB2uq09PfZ"
      },
      "source": [
        "## Prepare prompt function <br> returns: messages, response_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDsBrnh4w943",
        "outputId": "62846e7b-e227-4162-ccb1-dc893ea7d34a"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "def prepare_prompt_en(symptoms, details):\n",
        "    prompt= f\"\"\"\n",
        "    A patient reports these dental symptoms: {', '.join(symptoms)}.\n",
        "    Additional details: {details}.\n",
        "\n",
        "    Provide the 3 most probable dental conditions with confidence percentages.\n",
        "    \"\"\"\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful dentist assistant that diagnoses conditions based on symptoms and that outputs in JSON.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt},\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def prepare_prompt_fr(symptoms, details):\n",
        "    prompt= f\"\"\"\n",
        "    A patient reports these dental symptoms: {', '.join(symptoms)}.\n",
        "    Additional details: {details}.\n",
        "\n",
        "    Provide the 3 most probable dental conditions in french with confidence percentages.\n",
        "    \"\"\"\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful dentist assistant that speaks french and diagnoses conditions based on symptoms and that outputs in JSON.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt},\n",
        "    ]\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mea25cwB99cX"
      },
      "source": [
        "### Add System prompt and append user prompt (Not used but could be useful to simplify functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG2UAXyh-GwN"
      },
      "outputs": [],
      "source": [
        "# %%writefile -a app.py\n",
        "# def add_system_prompt_to_chat_history(system_prompt, chat_history_without_system_prompt):\n",
        "#   messages=[]\n",
        "#   system_prompt_element={\n",
        "#       \"role\": \"user\",\n",
        "#       \"content\": system_prompt\n",
        "#   }\n",
        "\n",
        "#   messages.append(system_prompt_element)\n",
        "#   chat_history_with_system_prompt= messages + chat_history_without_system_prompt\n",
        "\n",
        "#   return chat_history_with_system_prompt\n",
        "\n",
        "# def append_user_prompt_to_chat_history(chat_history_with_system_prompt, prompt):\n",
        "#   prompt_element={\n",
        "#       \"role\": \"user\",\n",
        "#       \"content\": prompt\n",
        "#   }\n",
        "\n",
        "#   chat_history_with_system_prompt.append(prompt_element)\n",
        "\n",
        "#   return chat_history_with_system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xDNGiDsAhcJ"
      },
      "source": [
        "### Helper functions for summary and symptoms and conditions (They do the actual work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEanZjExAoI0",
        "outputId": "6f754c2d-6d1f-4caa-eaa2-aac89d4f298b"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "def extract_symptoms_from_history(chat_history: list[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    Extracts symptoms from the chat history using the LLM.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the extracted symptoms (e.g., {\"symptoms\": [...]})\n",
        "    \"\"\"\n",
        "    symptoms_system_prompt = \"You are a helpful dentist assistant that extracts symptoms based on conversations and exports to JSON.\"\n",
        "    symptoms_user_prompt = \"Can you please extract patient symptoms from the conversation for the doctor to review\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": symptoms_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": symptoms_user_prompt}]\n",
        "    )\n",
        "    response_format = get_symptoms_json_response_format()\n",
        "    response = query_local_model(messages, response_format)\n",
        "    return response\n",
        "\n",
        "def extract_conditions_from_history(chat_history: list[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    Extracts conditions from the chat history using the LLM.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the extracted conditions (e.g., {\"diagnosis\": [...]})\n",
        "    \"\"\"\n",
        "    conditions_system_prompt = \"You are a helpful dentist assistant that determines dental conditions based on conversations and exports to JSON.\"\n",
        "    conditions_user_prompt = \"Can you please determine what are the most probable dental conditions based on what I told you along with confidence percentages for each condition\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": conditions_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": conditions_user_prompt}]\n",
        "    )\n",
        "    response_format = get_conditions_json_response_format()\n",
        "    response = query_local_model(messages, response_format)\n",
        "    return response\n",
        "\n",
        "def summarize_chat_history(chat_history: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Summarizes the chat history into a concise text summary.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the summary.\n",
        "    \"\"\"\n",
        "    summary_system_prompt = \"You are a helpful dentist assistant that summarizes patient concerns, habits and medical history if possible based on conversations between a patient and an LLM.\"\n",
        "    summary_user_prompt = \"Can you please summarize the whole conversation for the doctor without including any symptoms or conditions. Don't say anything apart from the summary\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": summary_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": summary_user_prompt}]\n",
        "    )\n",
        "    summary = query_local_model(messages)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6B9tN-9lzGz"
      },
      "source": [
        "## Get json response format functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9tZpKRRl5jL",
        "outputId": "ebe2bbe0-35c1-43ae-fc38-3290d0b0b2dd"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "def get_conditions_json_response_format():\n",
        "    response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"diagnosis\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"condition\": {\"type\": \"string\"},\n",
        "                            \"confidence\": {\"type\": \"integer\"}\n",
        "                            },\n",
        "                        \"required\": [\"condition\",\"confidence\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 3\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"diagnosis\"]\n",
        "            }\n",
        "        }\n",
        "    return response_format\n",
        "\n",
        "def get_symptoms_json_response_format():\n",
        "  response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"symptoms\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"symptom\": {\"type\": \"string\"}\n",
        "                            },\n",
        "                        \"required\": [\"symptom\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 10\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"symptoms\"]\n",
        "            }\n",
        "        }\n",
        "  return response_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HcKXMlhwLY1"
      },
      "source": [
        "## Upload ngrok url to gist function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA17aXcYvTlY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def update_ngrok_url_in_gist(ngrok_url):\n",
        "  GIST_ID = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Replace with your gist ID\n",
        "  GITHUB_TOKEN = \"github_pat_token_xxx\"  # Replace with your token\n",
        "  headers = {\n",
        "      \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
        "      \"Accept\": \"application/vnd.github.v3+json\"\n",
        "  }\n",
        "  data = {\n",
        "      \"files\": {\n",
        "          \"ngrok_url.txt\": {\n",
        "              \"content\": ngrok_url\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "  response = requests.patch(\n",
        "      f\"https://api.github.com/gists/{GIST_ID}\",\n",
        "      headers=headers,\n",
        "      data=json.dumps(data)\n",
        "  )\n",
        "  if response.status_code == 200:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfL-l2kE_M8y"
      },
      "source": [
        "## Query local model method <br> Calls the llm.create_chat_completions method and returns the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAzRZqjd_bvG",
        "outputId": "fccf12e2-1397-4ae0-9211-70e9b4c3752b"
      },
      "outputs": [],
      "source": [
        "%%writefile -a app.py\n",
        "def query_local_model(messages, response_format=None):\n",
        "  output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    response_format=response_format,\n",
        "    temperature=0.3,\n",
        "    max_tokens=256,\n",
        "    )\n",
        "  content = output[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "  # If response_format is provided, parse as JSON; otherwise, return as plain text\n",
        "  if response_format:\n",
        "    try:\n",
        "      return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "      return {\"error\": \"Failed to parse JSON response\", \"output\": content}\n",
        "  else:\n",
        "    return content\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.set_auth_token(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your token\n",
        "    # public_url = ngrok.connect(8000).public_url\n",
        "    # print(f\" * Test all endpoints interactively at: {public_url}/docs\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Iyc-EUDWAjh"
      },
      "source": [
        "# Lancer le serveur\n",
        "## Cliquer sur l'url affiché (qui se termine par /docs) pour tester l'api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBSQUGLC4laj"
      },
      "outputs": [],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b40Zp0twkx3",
        "outputId": "f1b4cf27-8021-43cb-f64c-31b74d4a5764"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the FastAPI server in the background\n",
        "process = subprocess.Popen([\"python\", \"app.py\"])\n",
        "\n",
        "# Expose the server using ngrok\n",
        "ngrok.set_auth_token(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your token\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "update_response = update_ngrok_url_in_gist(public_url)\n",
        "print(f\" * Updated Gist with new ngrok URL: {update_response}\")\n",
        "print(f\" * ngrok tunnel/API URL running at: {public_url}\")\n",
        "print(f\" * Test all endpoints interactively at: {public_url}/docs\")\n",
        "# print(f\"Nb!:Il faut remplacer NGROK_URL dans .env dans le projet FastAPI par cet url\")\n",
        "\n",
        "# Keep the cell alive to prevent the process from terminating\n",
        "try:\n",
        "    while True:\n",
        "        pass\n",
        "except KeyboardInterrupt:\n",
        "    process.terminate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bhFEIhM6mXj"
      },
      "source": [
        "# Testing summarize the patient consultation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Pd-jtDgFC9Z3",
        "outputId": "ca35ad27-9fb3-4a15-8404-c48038dfcd59"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "model_path = \"models/qwen2.5-7b-q4_k_m.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=-1)\n",
        "prompt = \"Can you please summarize the whole conversation for the doctor without including any symptoms or conditions. Don't say anything apart from the summary\"\n",
        "messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are a helpful dentist assistant that summarizes patient concerns, habits and medical history if possible based on conversations between a patient and an LLM.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"hi\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Hello! I'm here to help with any dental concerns you might have. Could you describe what's bringing you in today? Are you experiencing any pain, discomfort, or noticing something unusual with your teeth or gums?\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"yes i'm feeling pain in my teeth especially when i eat cold things\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for sharing that with me. The pain you're experiencing when eating cold things could be a sign of several dental issues. To help diagnose the problem, could you answer a few questions?\\n\\n1. How long have you been experiencing this pain?\\n2. Do you have any other symptoms like sensitivity to heat, discoloration of the tooth, or swelling in the gums?\\n3. Have you noticed any changes in your teeth recently, such as chips or cracks?\\n4. Do you have any fillings or crowns in your teeth?\\n5. How often do you brush your teeth, and do you use a soft-bristled toothbrush?\\n\\nThis information will help me better understand your situation.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"i use a soft toothbrush and brush regularly and the pain started about 2 weeks ago and it's only getting worse along with swelling in my gums\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for providing those additional details. Based on what you've described, the combination of pain, especially with cold things, swelling in the gums, and the recent onset of symptoms could indicate a few potential issues:\\n\\n1. **Dental Decay (Cavities):** The pain when eating cold things could be due to decay that has reached the pulp of the tooth, causing inflammation and sensitivity.\\n\\n2. **Gum Infection (Gingivitis or Periodontitis):** The swelling in your gums could be due to an infection, which is worsening over time. This could be related to poor plaque removal or other underlying issues.\\n\\n3. **Tooth Fracture or Crack:** If you've had any trauma to your teeth, a crack might have developed, leading to sensitivity and pain, especially with cold stimuli.\\n\\n4. **Reversible Pulpitis:** This is inflammation of the tooth's pulp that can be reversible if caught early. However, if left untreated, it can lead to irreversible pulpitis or an abscess.\\n\\nGiven that the pain is worsening and now includes swelling, it's important to see a dentist as soon as possible. The swelling could indicate an infection that needs to be addressed promptly to prevent further complications.\\n\\nWould you be able to make an appointment with a dentist soon? They will be able to perform a thorough examination and possibly take X-rays to determine the exact cause of your symptoms. In the meantime, you can try to avoid very cold foods and drinks to minimize discomfort.\\n\\nIf you have any more questions or need further assistance, feel free to ask!\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "    }\n",
        "  ]\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.4,\n",
        "    max_tokens=128,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdNsVy3bDs_3",
        "outputId": "f37172b7-9985-4618-dfca-21c5ad536513"
      },
      "outputs": [],
      "source": [
        "print(output[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Pa64HU5Ty-"
      },
      "source": [
        "# Testing extract symptoms from chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrNuiJ5n5W7S",
        "outputId": "732a9520-fec5-49a6-9517-96406108dcf7"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "model_path = \"models/qwen2.5-7b-q4_k_m.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=-1)\n",
        "prompt = \"Can you please extract patient symptoms from the conversation for the doctor to review\"\n",
        "messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are a helpful dentist assistant that extracts symptoms based on conversations and exports to JSON.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"hi\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Hello! I'm here to help with any dental concerns you might have. Could you describe what's bringing you in today? Are you experiencing any pain, discomfort, or noticing something unusual with your teeth or gums?\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"yes i'm feeling pain in my teeth especially when i eat cold things\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for sharing that with me. The pain you're experiencing when eating cold things could be a sign of several dental issues. To help diagnose the problem, could you answer a few questions?\\n\\n1. How long have you been experiencing this pain?\\n2. Do you have any other symptoms like sensitivity to heat, discoloration of the tooth, or swelling in the gums?\\n3. Have you noticed any changes in your teeth recently, such as chips or cracks?\\n4. Do you have any fillings or crowns in your teeth?\\n5. How often do you brush your teeth, and do you use a soft-bristled toothbrush?\\n\\nThis information will help me better understand your situation.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"i use a soft toothbrush and brush regularly and the pain started about 2 weeks ago and it's only getting worse along with swelling in my gums\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for providing those additional details. Based on what you've described, the combination of pain, especially with cold things, swelling in the gums, and the recent onset of symptoms could indicate a few potential issues:\\n\\n1. **Dental Decay (Cavities):** The pain when eating cold things could be due to decay that has reached the pulp of the tooth, causing inflammation and sensitivity.\\n\\n2. **Gum Infection (Gingivitis or Periodontitis):** The swelling in your gums could be due to an infection, which is worsening over time. This could be related to poor plaque removal or other underlying issues.\\n\\n3. **Tooth Fracture or Crack:** If you've had any trauma to your teeth, a crack might have developed, leading to sensitivity and pain, especially with cold stimuli.\\n\\n4. **Reversible Pulpitis:** This is inflammation of the tooth's pulp that can be reversible if caught early. However, if left untreated, it can lead to irreversible pulpitis or an abscess.\\n\\nGiven that the pain is worsening and now includes swelling, it's important to see a dentist as soon as possible. The swelling could indicate an infection that needs to be addressed promptly to prevent further complications.\\n\\nWould you be able to make an appointment with a dentist soon? They will be able to perform a thorough examination and possibly take X-rays to determine the exact cause of your symptoms. In the meantime, you can try to avoid very cold foods and drinks to minimize discomfort.\\n\\nIf you have any more questions or need further assistance, feel free to ask!\"\n",
        "    },\n",
        "        {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"symptoms\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"symptom\": {\"type\": \"string\"}\n",
        "                            },\n",
        "                        \"required\": [\"symptom\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 10\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"symptoms\"]\n",
        "            }\n",
        "        }\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    response_format=response_format,\n",
        "    temperature=0.4,\n",
        "    max_tokens=256,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5opxkYPj5daR",
        "outputId": "143b6ef2-eb05-43e5-9c8b-a63e31e921fe"
      },
      "outputs": [],
      "source": [
        "print(output[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsjZYNVx2qKL"
      },
      "source": [
        "# Testing extract conditions from chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9Iq7YpL22tnh",
        "outputId": "f21263ad-a2d9-4d36-e26f-97866a254b7a"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model\n",
        "model_path = \"models/qwen2.5-7b-q4_k_m.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=-1)\n",
        "prompt = \"Can you please extract the conditions the patient is feeling from the conversation for the doctor\"\n",
        "messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are a helpful dentist assistant that determines dental conditions based on conversations and exports to JSON.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"hi\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Hello! I'm here to help with any dental concerns you might have. Could you describe what's bringing you in today? Are you experiencing any pain, discomfort, or noticing something unusual with your teeth or gums?\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"yes i'm feeling pain in my teeth especially when i eat cold things\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for sharing that with me. The pain you're experiencing when eating cold things could be a sign of several dental issues. To help diagnose the problem, could you answer a few questions?\\n\\n1. How long have you been experiencing this pain?\\n2. Do you have any other symptoms like sensitivity to heat, discoloration of the tooth, or swelling in the gums?\\n3. Have you noticed any changes in your teeth recently, such as chips or cracks?\\n4. Do you have any fillings or crowns in your teeth?\\n5. How often do you brush your teeth, and do you use a soft-bristled toothbrush?\\n\\nThis information will help me better understand your situation.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"i use a soft toothbrush and brush regularly and the pain started about 2 weeks ago and it's only getting worse along with swelling in my gums\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Thank you for providing those additional details. Based on what you've described, the combination of pain, especially with cold things, swelling in the gums, and the recent onset of symptoms could indicate a few potential issues:\\n\\n1. **Dental Decay (Cavities):** The pain when eating cold things could be due to decay that has reached the pulp of the tooth, causing inflammation and sensitivity.\\n\\n2. **Gum Infection (Gingivitis or Periodontitis):** The swelling in your gums could be due to an infection, which is worsening over time. This could be related to poor plaque removal or other underlying issues.\\n\\n3. **Tooth Fracture or Crack:** If you've had any trauma to your teeth, a crack might have developed, leading to sensitivity and pain, especially with cold stimuli.\\n\\n4. **Reversible Pulpitis:** This is inflammation of the tooth's pulp that can be reversible if caught early. However, if left untreated, it can lead to irreversible pulpitis or an abscess.\\n\\nGiven that the pain is worsening and now includes swelling, it's important to see a dentist as soon as possible. The swelling could indicate an infection that needs to be addressed promptly to prevent further complications.\\n\\nWould you be able to make an appointment with a dentist soon? They will be able to perform a thorough examination and possibly take X-rays to determine the exact cause of your symptoms. In the meantime, you can try to avoid very cold foods and drinks to minimize discomfort.\\n\\nIf you have any more questions or need further assistance, feel free to ask!\"\n",
        "    },\n",
        "        {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"diagnosis\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"condition\": {\"type\": \"string\"},\n",
        "                            \"confidence\": {\"type\": \"integer\"}\n",
        "                            },\n",
        "                        \"required\": [\"condition\",\"confidence\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 3\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"diagnosis\"]\n",
        "            }\n",
        "        }\n",
        "\n",
        "output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    response_format=response_format,\n",
        "    temperature=0.4,\n",
        "    max_tokens=256,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBH85jxo41Qm",
        "outputId": "1769dc40-1256-4482-aeb9-7687a7fd9c16"
      },
      "outputs": [],
      "source": [
        "print(output[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t46SUDKoo7eu"
      },
      "source": [
        "# Experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZWON1iSAr-s"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyxMj1GEH8kV",
        "outputId": "c9a45edb-2ba0-4a7d-d433-6e694ef7fa71"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
