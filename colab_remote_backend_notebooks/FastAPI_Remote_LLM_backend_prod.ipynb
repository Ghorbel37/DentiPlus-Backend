{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wDphgreJ7d6"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG-O6cwAJtAu"
      },
      "source": [
        "## Download required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3G_EHgQwXHE",
        "outputId": "aa0b5a50-8198-4221-e2da-542c33ecef5e"
      },
      "outputs": [],
      "source": [
        "!pip install -q fastapi uvicorn pyngrok pydantic\n",
        "!pip install -q llama-cpp-python==0.3.16 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E8jwuPAJw9C"
      },
      "source": [
        "## Download model from hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT6QBxc6wjyu",
        "outputId": "d7f8227a-193f-4684-fb33-731409e6a1b3"
      },
      "outputs": [],
      "source": [
        "!mkdir -p models\n",
        "!wget -O models/qwen2.5-7b-q4_k_m.gguf https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/resolve/main/Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRLLOEiYJ2Ch"
      },
      "source": [
        "## Write script to app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDsBrnh4w943",
        "outputId": "b6a4f885-d2ba-421f-c4f4-62a48c31116d"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import random\n",
        "from pyngrok import ngrok\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from llama_cpp import Llama\n",
        "import uvicorn\n",
        "import json\n",
        "\n",
        "# Load the model\n",
        "model_path = \"models/qwen2.5-7b-q4_k_m.gguf\"\n",
        "llm = Llama(model_path=model_path, n_ctx=2048, n_threads=4, n_gpu_layers=-1)\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, llm: Llama, system_prompt=\"\", history=[]):\n",
        "        self.llm = llm\n",
        "        self.system_prompt = system_prompt\n",
        "        self.history = [{\"role\": \"system\", \"content\": self.system_prompt}] + history\n",
        "    def create_completion(self, user_prompt=''):\n",
        "        # Add the user prompt to the history\n",
        "        self.history.append({\"role\": \"user\", \"content\": user_prompt})\n",
        "        # Send the history messages to the LLM\n",
        "        output = self.llm.create_chat_completion(messages=self.history, temperature=0.3, max_tokens=400)\n",
        "        conversation_result = output['choices'][0]['message']\n",
        "        # Append the conversation_result to the history\n",
        "        self.history.append(conversation_result)\n",
        "        return conversation_result['content']\n",
        "\n",
        "    def final_response(self, response_format):\n",
        "        output = self.llm.create_chat_completion(messages=self.history, response_format=response_format)\n",
        "        try:\n",
        "            return json.loads(output[\"choices\"][0][\"message\"][\"content\"].strip())  # Parse JSON response\n",
        "        except json.JSONDecodeError:\n",
        "            return [{\"condition\": \"Error parsing response\", \"confidence\": \"N/A\",\"output\":output}]\n",
        "\n",
        "    def delete_history(self):\n",
        "        self.history=[{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history\n",
        "\n",
        "system_prompt=\"You are a helpful dentist assistant that asks precise questions to diagnose dental conditions based on conversations.\"\n",
        "chatbot = Conversation(llm, system_prompt=system_prompt)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"http://localhost:5173\", \"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class SymptomRequest(BaseModel):\n",
        "    symptoms: list[str]\n",
        "    additional_details: str = \"None\"\n",
        "\n",
        "class DiagnosisResponse(BaseModel):\n",
        "    diagnosis: list[dict]\n",
        "\n",
        "class ChatHistoryRequest(BaseModel):\n",
        "    chat_history: list[dict]\n",
        "\n",
        "class SymptomsResponse(BaseModel):\n",
        "    symptoms: list[dict]\n",
        "\n",
        "class ConditionsResponse(BaseModel):\n",
        "    diagnosis: list[dict]\n",
        "\n",
        "class SummaryResponse(BaseModel):\n",
        "    summary: str\n",
        "\n",
        "class CombinedResponse(BaseModel):\n",
        "    symptoms: list[dict]\n",
        "    conditions: list[dict]\n",
        "    summary: str\n",
        "\n",
        "# Request model for the note improvement endpoint\n",
        "class ImproveNoteRequest(BaseModel):\n",
        "    etat: str\n",
        "    doctor_note: str\n",
        "    chat_history: list[dict]\n",
        "\n",
        "# Response model for the improved note\n",
        "class ImprovedNoteResponse(BaseModel):\n",
        "    improved_note: str\n",
        "\n",
        "@app.get(\"/hello\")\n",
        "async def root():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/diagnose-en\", response_model=DiagnosisResponse)\n",
        "async def diagnose_patient_lm(request: SymptomRequest):\n",
        "    try:\n",
        "        messages = prepare_prompt_en(request.symptoms, request.additional_details)\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response = query_local_model(messages, response_format)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/diagnose-fr\", response_model=DiagnosisResponse)\n",
        "async def diagnose_patient_fr(request: SymptomRequest):\n",
        "    try:\n",
        "        messages = prepare_prompt_fr(request.symptoms, request.additional_details)\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response = query_local_model(messages, response_format)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/converse\")\n",
        "async def converse(request):\n",
        "    try:\n",
        "        response=chatbot.create_completion(request)\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/converse_diagnose\")\n",
        "async def converse_diagnose():\n",
        "    try:\n",
        "        response_format = get_conditions_json_response_format()\n",
        "        response=chatbot.final_response(response_format)\n",
        "        chatbot.delete_history()\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/converse_history\")\n",
        "async def converse_history():\n",
        "    try:\n",
        "        response = chatbot.get_history()\n",
        "        return {\"history\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/random_greeting\")\n",
        "async def get_random_greeting():\n",
        "    greetings=[\"Hello! Welcome to our practice. How can I assist you today?\",\n",
        "           \"Good morning/afternoon! Thank you for visiting us. What brings you in today?\",\n",
        "           \"Hi there! I hope you're feeling well. How can I help you?\",\n",
        "           \"Hello! We're glad you're here. What can I do for you today?\",\n",
        "           \"Good day! I see you're here. What would you like to discuss?\",\n",
        "           \"Hi! Thank you for coming in. How can I assist you?\",\n",
        "           \"Hello! We're here to help. What can I do for you today?\",\n",
        "           \"Hi! I hope you're feeling well. What can I do for you today?\",\n",
        "           \"Good day! We're glad you're here. What brings you in today?\"]\n",
        "    try:\n",
        "        return {\"greeting\": random.choice(greetings)}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/extract_symptoms\", response_model=SymptomsResponse)\n",
        "async def extract_symptoms(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        response = extract_symptoms_from_history(request.chat_history)\n",
        "        return {\"symptoms\": response[\"symptoms\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/extract_conditions\", response_model=ConditionsResponse)\n",
        "async def extract_conditions(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        response = extract_conditions_from_history(request.chat_history)\n",
        "        return {\"diagnosis\": response[\"diagnosis\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/summarize\", response_model=SummaryResponse)\n",
        "async def summarize_chat(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        SummaryResponse.summary = summarize_chat_history(request.chat_history)\n",
        "        return SummaryResponse\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Updated /process_chat endpoint using helper functions\n",
        "@app.post(\"/process_chat\", response_model=CombinedResponse)\n",
        "async def process_chat(request: ChatHistoryRequest):\n",
        "    try:\n",
        "        # Extract all required data using helper functions\n",
        "        symptoms_response = extract_symptoms_from_history(request.chat_history)\n",
        "        conditions_response = extract_conditions_from_history(request.chat_history)\n",
        "        summary = summarize_chat_history(request.chat_history)\n",
        "\n",
        "        # Combine results\n",
        "        return CombinedResponse(\n",
        "            symptoms=symptoms_response.get(\"symptoms\", []),\n",
        "            conditions=conditions_response.get(\"diagnosis\", []),\n",
        "            summary=summary\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# New endpoint to improve the doctor's note\n",
        "@app.post(\"/improve_note\", response_model=ImprovedNoteResponse)\n",
        "async def improve_note(request: ImproveNoteRequest):\n",
        "    try:\n",
        "        improved_note = improve_doctor_note(\n",
        "            etat=request.etat,\n",
        "            doctor_note=request.doctor_note,\n",
        "            chat_history=request.chat_history\n",
        "        )\n",
        "        return {\"improved_note\": improved_note}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_with_model(request: ChatHistoryRequest):\n",
        "    chat_system_prompt = \"\"\"You are a helpful virtual assistant specialized in dental health. You only assist with diagnosing dental conditions by asking the patient precise and relevant questions.\n",
        "    If the user asks anything not related to dental health, politely refuse to answer and remind them that you are only trained to assist with dental diagnoses.\"\"\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": chat_system_prompt}]\n",
        "        + request.chat_history\n",
        "    )\n",
        "    response = query_local_model(messages)\n",
        "    return {\"response\": response}\n",
        "\n",
        "def prepare_prompt_en(symptoms, details):\n",
        "    prompt= f\"\"\"\n",
        "    A patient reports these dental symptoms: {', '.join(symptoms)}.\n",
        "    Additional details: {details}.\n",
        "\n",
        "    Provide the 3 most probable dental conditions with confidence percentages.\n",
        "    \"\"\"\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful dentist assistant that diagnoses conditions based on symptoms and that outputs in JSON.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt},\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def prepare_prompt_fr(symptoms, details):\n",
        "    prompt= f\"\"\"\n",
        "    A patient reports these dental symptoms: {', '.join(symptoms)}.\n",
        "    Additional details: {details}.\n",
        "\n",
        "    Provide the 3 most probable dental conditions in french with confidence percentages.\n",
        "    \"\"\"\n",
        "\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful dentist assistant that speaks french and diagnoses conditions based on symptoms and that outputs in JSON.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt},\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "def get_conditions_json_response_format():\n",
        "    response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"diagnosis\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"condition\": {\"type\": \"string\"},\n",
        "                            \"confidence\": {\"type\": \"integer\"}\n",
        "                            },\n",
        "                        \"required\": [\"condition\",\"confidence\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 3\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"diagnosis\"]\n",
        "            }\n",
        "        }\n",
        "    return response_format\n",
        "\n",
        "def get_symptoms_json_response_format():\n",
        "  response_format={\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"symptoms\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                        \"type\":\"object\",\n",
        "                        \"properties\": {\n",
        "                            \"symptom\": {\"type\": \"string\"}\n",
        "                            },\n",
        "                        \"required\": [\"symptom\"],\n",
        "                        },\n",
        "                    \"minItems\": 1,\n",
        "                    \"maxItems\": 10\n",
        "                    },\n",
        "                },\n",
        "            \"required\": [\"symptoms\"]\n",
        "            }\n",
        "        }\n",
        "  return response_format\n",
        "\n",
        "def extract_symptoms_from_history(chat_history: list[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    Extracts symptoms from the chat history using the LLM.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the extracted symptoms (e.g., {\"symptoms\": [...]})\n",
        "    \"\"\"\n",
        "    symptoms_system_prompt = \"You are a helpful dentist assistant that extracts symptoms based on conversations and exports to JSON.\"\n",
        "    symptoms_user_prompt = \"Can you please extract patient symptoms from the conversation for the doctor to review\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": symptoms_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": symptoms_user_prompt}]\n",
        "    )\n",
        "    response_format = get_symptoms_json_response_format()\n",
        "    response = query_local_model(messages, response_format)\n",
        "    return response\n",
        "\n",
        "def extract_conditions_from_history(chat_history: list[dict]) -> dict:\n",
        "    \"\"\"\n",
        "    Extracts conditions from the chat history using the LLM.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing the extracted conditions (e.g., {\"diagnosis\": [...]})\n",
        "    \"\"\"\n",
        "    conditions_system_prompt = \"You are a helpful dentist assistant that determines dental conditions based on conversations and exports to JSON.\"\n",
        "    conditions_user_prompt = \"Can you please determine what are the most probable dental conditions based on what I told you along with confidence percentages for each condition\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": conditions_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": conditions_user_prompt}]\n",
        "    )\n",
        "    response_format = get_conditions_json_response_format()\n",
        "    response = query_local_model(messages, response_format)\n",
        "    return response\n",
        "\n",
        "def summarize_chat_history(chat_history: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Summarizes the chat history into a concise text summary.\n",
        "\n",
        "    Args:\n",
        "        chat_history: List of messages with 'role' and 'content' keys.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the summary.\n",
        "    \"\"\"\n",
        "    summary_system_prompt = \"You are a helpful dentist assistant that summarizes patient concerns, habits and medical history if possible based on conversations between a patient and an LLM.\"\n",
        "    summary_user_prompt = \"Can you please summarize the whole conversation for the doctor without including any symptoms or conditions. Don't say anything apart from the summary\"\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": summary_system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": summary_user_prompt}]\n",
        "    )\n",
        "    summary = query_local_model(messages)\n",
        "    return summary\n",
        "\n",
        "# Helper function to improve the doctor's note\n",
        "def improve_doctor_note(etat: str, doctor_note: str, chat_history: list[dict]) -> str:\n",
        "    system_prompt = (\n",
        "        \"You are a professional medical assistant specializing in refining doctor’s notes. \"\n",
        "        \"Your task is to improve the clarity, grammar, and readability of the provided doctor’s note \"\n",
        "        \"while preserving its original meaning and intent. Do not add new information, remove content, \"\n",
        "        \"or alter the medical conclusions. Fix typos, adjust awkward phrasing, and ensure a polished, \"\n",
        "        \"professional tone. Use the consultation status and conversation for context, but do not incorporate \"\n",
        "        \"details from them into the note unless they are already present in the doctor’s note.\"\n",
        "    )\n",
        "    user_prompt = (\n",
        "        f\"The consultation has the status: {etat}.\\n\"\n",
        "        f\"Here is the doctor’s note to improve:\\n{doctor_note}\\n\"\n",
        "        \"Please provide the improved version of the note, keeping the meaning unchanged.\"\n",
        "    )\n",
        "    messages = (\n",
        "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        + chat_history\n",
        "        + [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "    )\n",
        "    return query_local_model(messages)  # Returns plain text\n",
        "\n",
        "def query_local_model(messages, response_format=None):\n",
        "  output = llm.create_chat_completion(\n",
        "    messages=messages,\n",
        "    response_format=response_format,\n",
        "    temperature=0.3,\n",
        "    max_tokens=256,\n",
        "    )\n",
        "  content = output[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "  if response_format:\n",
        "    try:\n",
        "      return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "      return {\"error\": \"Failed to parse JSON response\", \"output\": content}\n",
        "  else:\n",
        "    return content\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ngrok.set_auth_token(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your token\n",
        "    # public_url = ngrok.connect(8000).public_url\n",
        "    # print(f\" * Test all endpoints interactively at: {public_url}/docs\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA-0_Sl72HOF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def update_ngrok_url_in_gist(ngrok_url):\n",
        "  GIST_ID = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Replace with your gist ID\n",
        "  GITHUB_TOKEN = \"github_pat_token_xxx\"  # Replace with your token\n",
        "  headers = {\n",
        "      \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
        "      \"Accept\": \"application/vnd.github.v3+json\"\n",
        "  }\n",
        "  data = {\n",
        "      \"files\": {\n",
        "          \"ngrok_url.txt\": {\n",
        "              \"content\": ngrok_url\n",
        "          }\n",
        "      }\n",
        "  }\n",
        "  response = requests.patch(\n",
        "      f\"https://api.github.com/gists/{GIST_ID}\",\n",
        "      headers=headers,\n",
        "      data=json.dumps(data)\n",
        "  )\n",
        "  # print(f\" * Updated Gist with new ngrok tunnel URL running at: {ngrok_url}\")\n",
        "  if response.status_code == 200:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Iyc-EUDWAjh"
      },
      "source": [
        "# Lancer le serveur\n",
        "## Cliquer sur l'url affiché (qui se termine par /docs) pour tester l'api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b40Zp0twkx3",
        "outputId": "bb02d0c6-7771-4d30-f9fc-6ef80eb87a85"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the FastAPI server in the background\n",
        "process = subprocess.Popen([\"python\", \"app.py\"])\n",
        "\n",
        "# Expose the server using ngrok\n",
        "ngrok.set_auth_token(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  # Replace with your token\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "update_response = update_ngrok_url_in_gist(public_url)\n",
        "print(f\" * Updated Gist with new ngrok URL: {update_response}\")\n",
        "print(f\" * ngrok tunnel running at: {public_url}\")\n",
        "print(f\" * Test all endpoints interactively at: {public_url}/docs\")\n",
        "# print(f\"Nb!:Il faut remplacer NGROK_URL dans .env dans le projet FastAPI par cet url\")\n",
        "\n",
        "# Keep the cell alive to prevent the process from terminating\n",
        "try:\n",
        "    while True:\n",
        "        pass\n",
        "except KeyboardInterrupt:\n",
        "    process.terminate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
